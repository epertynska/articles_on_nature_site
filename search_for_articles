import requests
from bs4 import BeautifulSoup
import re
import string
import os

pages = int(input("How many first pages do you want to check?"))
kind = input("What kind of articles should I search for?")
counter = 1
current = os.getcwd()
while counter <= pages:
    page_response = requests.get('https://www.nature.com/nature/articles?sort=PubDate&year=2020&page=' + str(counter))

    main_soup = BeautifulSoup(page_response.content, 'html.parser')
    page = "Page_" + str(counter)
    os.mkdir(page)
    os.chdir(page)

    links = []
    marks = [a for a in string.punctuation]

    try:
        article_soup = main_soup.findAll('span', {'class': 'c-meta__type'}, text=kind)

        for news_article in article_soup:
            links.append('https://www.nature.com' + news_article.find_parent('article').find('a', attrs={
                'href': re.compile("articles")}).get('href'))

        for url in links:
            page_response_2 = requests.get(url)

            article_soup = BeautifulSoup(page_response_2.content, 'html.parser')

            article_title_soup = article_soup.find('h1', class_="c-article-magazine-title").text.strip()
            str_list = [a for a in article_title_soup if a not in marks]

            article_file_name = (''.join(str_list)).replace(' ', '_') + '.txt'

            body_text = article_soup.find('div', class_='c-article-body u-clearfix').text

            with open(article_file_name, 'w', encoding="UTF-8") as article_file:
                article_file.write(body_text)
    except:
        continue

    os.chdir(current)
    counter += 1
print("Saved all articles.")
